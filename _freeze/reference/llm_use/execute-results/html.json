{
  "hash": "a796c6e26467e312d351113326064d61",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Specify the model to use\"\nexecute:\n  eval: true\n  freeze: true\n---\n\n\n\n\n\n[R/llm-use.R](https://github.com/edgararuiz/mall/blob/main/R/llm-use.R)\n\n## llm_use\n\n## Description\n Allows us to specify the back-end provider, model to use during the current R session \n\n\n## Usage\n```r\n \nllm_use( \n  backend = NULL, \n  model = NULL, \n  ..., \n  .silent = FALSE, \n  .cache = NULL, \n  .force = FALSE \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| backend | The name of an supported back-end provider. Currently only 'ollama' is supported. |\n| model | The name of model supported by the back-end provider |\n| ... | Additional arguments that this function will pass down to the integrating function. In the case of Ollama, it will pass those arguments to `ollamar::chat()`. |\n| .silent | Avoids console output |\n| .cache | The path to save model results, so they can be re-used if the same operation is ran again. To turn off, set this argument to an empty character: `\"\"`. 'It defaults to '_mall_cache'. If this argument is left `NULL` when calling this function, no changes to the path will be made. |\n| .force | Flag that tell the function to reset all of the settings in the R session |\n\n\n\n## Value\n A `mall_session` object \n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}