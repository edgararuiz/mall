{
  "hash": "84eedf7eec066709f406e09aee9d91c6",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Specify the model to use\"\nexecute:\n  eval: true\n  freeze: true\n---\n\n\n\n\n\n\n[R/llm-use.R](https://github.com/edgararuiz/mall/blob/main/R/llm-use.R)\n\n## llm_use\n\n## Description\n Allows us to specify the back-end provider, model to use during the current R session \n\n\n## Usage\n```r\n \nllm_use( \n  backend = NULL, \n  model = NULL, \n  ..., \n  .silent = FALSE, \n  .cache = NULL, \n  .force = FALSE \n) \n```\n\n## Arguments\n|Arguments|Description|\n|---|---|\n| backend | The name of an supported back-end provider. Currently only 'ollama' is supported. |\n| model | The name of model supported by the back-end provider |\n| ... | Additional arguments that this function will pass down to the integrating function. In the case of Ollama, it will pass those arguments to `ollamar::chat()`. |\n| .silent | Avoids console output |\n| .cache | The path to save model results, so they can be re-used if the same operation is ran again. To turn off, set this argument to an empty character: `\"\"`. 'It defaults to '_mall_cache'. If this argument is left `NULL` when calling this function, no changes to the path will be made. |\n| .force | Flag that tell the function to reset all of the settings in the R session |\n\n\n\n## Value\n A `mall_session` object \n\n\n## Examples\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n \nlibrary(mall) \n \nllm_use(\"ollama\", \"llama3.2\") \n#> \n#> ── mall session object\n#> Backend: ollama\n#> LLM session: model:llama3.2\n#> R session: cache_folder:_mall_cache\n \n# Additional arguments will be passed 'as-is' to the \n# downstream R function in this example, to ollama::chat() \nllm_use(\"ollama\", \"llama3.2\", seed = 100, temp = 0.1) \n#> \n#> ── mall session object \n#> Backend: ollamaLLM session:  model:llama3.2\n#>   seed:100\n#>   temp:0.1\n#> R session: cache_folder:_mall_cache\n \n# During the R session, you can change any argument \n# individually and it will retain all of previous \n# arguments used \nllm_use(temp = 0.3) \n#> \n#> ── mall session object \n#> Backend: ollamaLLM session:  model:llama3.2\n#>   seed:100\n#>   temp:0.3\n#> R session: cache_folder:_mall_cache\n \n# Use .cache to modify the target folder for caching \nllm_use(.cache = \"_my_cache\") \n#> \n#> ── mall session object \n#> Backend: ollamaLLM session:  model:llama3.2\n#>   seed:100\n#>   temp:0.3\n#> R session: cache_folder:_my_cache\n \n# Leave .cache empty to turn off this functionality \nllm_use(.cache = \"\") \n#> \n#> ── mall session object \n#> Backend: ollamaLLM session:  model:llama3.2\n#>   seed:100\n#>   temp:0.3\n \n# Use .silent to avoid the print out \nllm_use(.silent = TRUE) \n \n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}