---
title: "Caching results"
execute:
  eval: true
  freeze: true
---

```{r}
#| include: false
library(fs)
dir_delete("_mall_cache")
source("../utils/knitr-print.R")
```

Data preparation, and model preparation, is usually a iterative process. Because
models in R are normally rather fast,   it is not a problem to re-run the
entire code to confirm that all of the results are reproducible. But in
the case of LLM's, re-running things may be a problem. Locally, running the 
LLM will be processor intensive, and typically long. If running against a remote
LLM, the issue would the cost per token. 

To ameliorate this, `mall` is able to cache existing results in a folder. That way, 
running the same analysis over and over, will be much quicker. Because instead of
calling the LLM again, `mall` will return the previously recorded result. 

By default, this functionality is turned on. The results will be saved to a folder
named "_mall_cache" . The name of the folder can be easily changed, simply set
the `.cache` argument in `llm_use()`. To **disable** this functionality, set
the argument to an empty character, meaning `.cache = ""`.

## How it works

`mall` uses all of the values used to make the LLM query as the "finger print"
to confidently identify when the same query is being done again. This includes:

- The value in the particular row
- The additional prompting built by the `llm_` function,
- Any other arguments/options used, set in `llm_use()`
- The name of the back end used for the call

## Examples 

We will initialize the LLM session specifying a seed

```{r}
library(mall)

llm_use("ollama", "llama3.1", seed = 100)
```
Using the `tictoc` package, we will measure how long it takes to make a simple
sentiment call. 

```{r}
library(tictoc)

tic()
llm_vec_sentiment("I am happy")
toc()
```

This creates a the "_mall_cache" folder, and inside a sub-folder, it creates a 
file with the cache. The name of the file is the resulting hash value of the
combination mentioned in the previous section. 

```{r}
dir_tree("_mall_cache", recurse = TRUE)
```

The cache is a JSON file, that contains both the call, and the response

```{r}
readLines("_mall_cache/08/086214f2638f60496fd0468d7de37c59.json")
```

Re-running the same `mall` call, will complete significantly faster

```{r}
tic()
llm_vec_sentiment("I am happy")
toc()
```

If a slightly different query is made, `mall` will recognize that this is a
different call, and it will send it to the LLM. The results are then saved in a 
new JSON file. 

```{r}
llm_vec_sentiment("I am very happy")

dir_tree("_mall_cache", recurse = TRUE)
```

```{r}
llm_use(seed = 101)
```
```{r}
llm_vec_sentiment("I am very happy")
```

```{r}
dir_tree("_mall_cache", recurse = TRUE)
```
