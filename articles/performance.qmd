---
title: "Performance"
execute:
  eval: true
  freeze: true
---

```{r}
#| include: false
library(dplyr)
mall::llm_use("ollama", "llama3.2", seed = 100, .cache = "_readme_cache")
source("../site/knitr-print.R")
```

## Performance

We will briefly cover this methods performance from two perspectives: 

- How long the analysis takes to run locally 

- How well it predicts 

To do so, we will use the `data_bookReviews` data set, provided by the `classmap`
package. For this exercise, only the first 100, of the total 1,000, are going
to be part of this analysis.

```{r}
library(mall)
library(classmap)
library(dplyr)

data(data_bookReviews)

data_bookReviews |>
  glimpse()
```
As per the docs, `sentiment` is a factor indicating the sentiment of the review:
negative (1) or positive (2)

```{r}
length(strsplit(paste(head(data_bookReviews$review, 100), collapse = " "), " ")[[1]])
```

Just to get an idea of how much data we're processing, I'm using a very, very 
simple word count. So we're analyzing a bit over 20 thousand words.

```{r}
reviews_llm <- data_bookReviews |>
  head(100) |> 
  llm_sentiment(
    col = review,
    options = c("positive" ~ 2, "negative" ~ 1),
    pred_name = "predicted"
  )
```

As far as **time**, on my Apple M3 machine, it took about 1.5 minutes to process,
100 rows, containing 20 thousand words. Setting `temp` to 0 in `llm_use()`, 
made the model run faster.

The package uses `purrr` to send each prompt individually to the LLM. But, I did
try a few different ways to speed up the process, unsuccessfully:

- Used `furrr` to send multiple requests at a time. This did not work because 
either the LLM or Ollama processed all my requests serially. So there was
no improvement.

- I also tried sending more than one row's text at a time. This cause instability
in the number of results. For example sending 5 at a time, sometimes returned 7
or 8. Even sending 2 was not stable. 

This is what the new table looks like:

```{r}
reviews_llm
```

I used `yardstick` to see how well the model performed. Of course, the accuracy
will not be of the "truth", but rather the package's results recorded in 
`sentiment`.

```{r}
library(forcats)

reviews_llm |>
  mutate(predicted = as.factor(predicted)) |>
  yardstick::accuracy(sentiment, predicted)
```
