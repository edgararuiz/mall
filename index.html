<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Run multiple Large Language Model predictions against a table, or vectors • mall</title>
<script src="lightswitch.js"></script><script src="deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="deps/font-awesome-6.4.2/css/all.min.css" rel="stylesheet">
<link href="deps/font-awesome-6.4.2/css/v4-shims.min.css" rel="stylesheet">
<script src="deps/headroom-0.11.0/headroom.min.js"></script><script src="deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="deps/search-1.0.0/fuse.min.js"></script><script src="deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="pkgdown.js"></script><meta property="og:title" content="Run multiple Large Language Model predictions against a table, or vectors">
<meta name="description" content="Run multiple Large Language Model predictions against a table. The predictions run row-wise over a specified column. It works using a pre-determined one-shot prompt, along with the current row's content. The prompt that is use will depend of the type of analysis needed.">
<meta property="og:description" content="Run multiple Large Language Model predictions against a table. The predictions run row-wise over a specified column. It works using a pre-determined one-shot prompt, along with the current row's content. The prompt that is use will depend of the type of analysis needed.">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top " aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="index.html">mall</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.0.0.9005</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="reference/index.html">Reference</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/edgararuiz/mall" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-lightswitch" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true" aria-label="Light switch"><span class="fa fa-sun"></span></button>
  <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="dropdown-lightswitch">
<li><button class="dropdown-item" data-bs-theme-value="light"><span class="fa fa-sun"></span> Light</button></li>
    <li><button class="dropdown-item" data-bs-theme-value="dark"><span class="fa fa-moon"></span> Dark</button></li>
    <li><button class="dropdown-item" data-bs-theme-value="auto"><span class="fa fa-adjust"></span> Auto</button></li>
  </ul>
</li>
      </ul>
</div>


  </div>
</nav><div class="container template-home">
<div class="row">
  <main id="main" class="col-md-9"><div class="section level1">
<div class="page-header"><h1 id="mall">mall<a class="anchor" aria-label="anchor" href="#mall"></a>
</h1></div>
<!-- badges: start -->

<!-- toc: start -->
<ul>
<li><a href="#motivation">Motivation</a></li>
<li>
<a href="#llm-functions">LLM functions</a>
<ul>
<li><a href="#sentiment">Sentiment</a></li>
<li><a href="#summarize">Summarize</a></li>
<li><a href="#classify">Classify</a></li>
<li><a href="#extract">Extract</a></li>
<li><a href="#translate">Translate</a></li>
<li><a href="#custom-prompt">Custom prompt</a></li>
</ul>
</li>
<li><a href="#initialize-session">Initialize session</a></li>
<li><a href="#key-considerations">Key considerations</a></li>
<li><a href="#performance">Performance</a></li>
<li><a href="#vector-functions">Vector functions</a></li>
<li><a href="#databricks">Databricks</a></li>
</ul>
<!-- toc: end --><div class="section level2">
<h2 id="intro">Intro<a class="anchor" aria-label="anchor" href="#intro"></a>
</h2>
<p>Run multiple LLM predictions against a table. The predictions run row-wise over a specified column. It works using a pre-determined one-shot prompt, along with the current row’s content. The prompt that is use will depend of the type of analysis needed. Currently, the included prompts perform the following:</p>
<ul>
<li>Sentiment analysis</li>
<li>Summarize the text</li>
<li>Extract one, or several, specific pieces information from the text</li>
</ul>
<p>This package is inspired by the SQL AI functions now offered by vendors such as <a href="https://docs.databricks.com/en/large-language-models/ai-functions.html" class="external-link">Databricks</a> and Snowflake. For local data, <code>mall</code> uses <a href="https://ollama.com/" class="external-link">Ollama</a> to call an LLM.</p>
<div class="section level3">
<h3 id="databricks-integration">Databricks integration<a class="anchor" aria-label="anchor" href="#databricks-integration"></a>
</h3>
<p>If you pass a table connected to <strong>Databricks</strong> via <code>odbc</code>, <code>mall</code> will automatically use Databricks’ LLM instead of Ollama. It will call the corresponding SQL AI function.</p>
</div>
</div>
<div class="section level2">
<h2 id="motivation">Motivation<a class="anchor" aria-label="anchor" href="#motivation"></a>
</h2>
<p>We want to new find ways to help data scientists use LLMs in their daily work. Unlike the familiar interfaces, such as chatting and code completion, this interface runs your text data directly against the LLM. The LLM’s flexibility, allows for it to adapt to the subject of your data, and provide surprisingly accurate predictions. This saves the data scientist the need to write and tune an NLP model.</p>
</div>
<div class="section level2">
<h2 id="llm-functions">LLM functions<a class="anchor" aria-label="anchor" href="#llm-functions"></a>
</h2>
<p>We will start with a very small table with product reviews:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://dplyr.tidyverse.org" class="external-link">dplyr</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">reviews</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://tibble.tidyverse.org/reference/tribble.html" class="external-link">tribble</a></span><span class="op">(</span></span>
<span>  <span class="op">~</span><span class="va">review</span>,</span>
<span>  <span class="st">"This has been the best TV I've ever used. Great screen, and sound."</span>,</span>
<span>  <span class="st">"I regret buying this laptop. It is too slow and the keyboard is too noisy"</span>,</span>
<span>  <span class="st">"Not sure how to feel about my new washing machine. Great color, but hard to figure"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="section level3">
<h3 id="sentiment">Sentiment<a class="anchor" aria-label="anchor" href="#sentiment"></a>
</h3>
<p>Primarily, <code>mall</code> provides verb-like functions that expect a <code>tbl</code> as their first argument. This allows us to use them in piped operations.</p>
<p>For the first example, we’ll asses the sentiment of each review. In order to do this we will call <code><a href="reference/llm_sentiment.html">llm_sentiment()</a></code>:</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://edgararuiz.github.io/mall/">mall</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">reviews</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/llm_sentiment.html">llm_sentiment</a></span><span class="op">(</span><span class="va">review</span><span class="op">)</span></span>
<span><span class="co">#&gt; # A tibble: 3 × 2</span></span>
<span><span class="co">#&gt;   review                                   .sentiment</span></span>
<span><span class="co">#&gt;   &lt;chr&gt;                                    &lt;chr&gt;     </span></span>
<span><span class="co">#&gt; 1 This has been the best TV I've ever use… positive  </span></span>
<span><span class="co">#&gt; 2 I regret buying this laptop. It is too … negative  </span></span>
<span><span class="co">#&gt; 3 Not sure how to feel about my new washi… neutral</span></span></code></pre></div>
<p>The function let’s us modify the options to choose from:</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">reviews</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/llm_sentiment.html">llm_sentiment</a></span><span class="op">(</span><span class="va">review</span>, options <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"positive"</span>, <span class="st">"negative"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; # A tibble: 3 × 2</span></span>
<span><span class="co">#&gt;   review                                   .sentiment</span></span>
<span><span class="co">#&gt;   &lt;chr&gt;                                    &lt;chr&gt;     </span></span>
<span><span class="co">#&gt; 1 This has been the best TV I've ever use… positive  </span></span>
<span><span class="co">#&gt; 2 I regret buying this laptop. It is too … negative  </span></span>
<span><span class="co">#&gt; 3 Not sure how to feel about my new washi… negative</span></span></code></pre></div>
<p>As mentioned before, by being pipe friendly, the results from the LLM prediction can be used in further transformations:</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">reviews</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/llm_sentiment.html">llm_sentiment</a></span><span class="op">(</span><span class="va">review</span>, options <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"positive"</span>, <span class="st">"negative"</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html" class="external-link">filter</a></span><span class="op">(</span><span class="va">.sentiment</span> <span class="op">==</span> <span class="st">"negative"</span><span class="op">)</span></span>
<span><span class="co">#&gt; # A tibble: 2 × 2</span></span>
<span><span class="co">#&gt;   review                                   .sentiment</span></span>
<span><span class="co">#&gt;   &lt;chr&gt;                                    &lt;chr&gt;     </span></span>
<span><span class="co">#&gt; 1 I regret buying this laptop. It is too … negative  </span></span>
<span><span class="co">#&gt; 2 Not sure how to feel about my new washi… negative</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="summarize">Summarize<a class="anchor" aria-label="anchor" href="#summarize"></a>
</h3>
<p>There may be a need to reduce the number of words in a given text. Usually, to make it easier to capture its intent. To do this, use <code><a href="reference/llm_summarize.html">llm_summarize()</a></code>. This function has an argument to control the maximum number of words to output (<code>max_words</code>):</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">reviews</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/llm_summarize.html">llm_summarize</a></span><span class="op">(</span><span class="va">review</span>, max_words <span class="op">=</span> <span class="fl">5</span><span class="op">)</span></span>
<span><span class="co">#&gt; # A tibble: 3 × 2</span></span>
<span><span class="co">#&gt;   review                                   .summary                       </span></span>
<span><span class="co">#&gt;   &lt;chr&gt;                                    &lt;chr&gt;                          </span></span>
<span><span class="co">#&gt; 1 This has been the best TV I've ever use… very good tv experience overall</span></span>
<span><span class="co">#&gt; 2 I regret buying this laptop. It is too … slow and noisy laptop purchase </span></span>
<span><span class="co">#&gt; 3 Not sure how to feel about my new washi… mixed feelings about new washer</span></span></code></pre></div>
<p>To control the name of the prediction field, you can change <code>pred_name</code> argument. This works with the other <code>llm_</code> functions as well.</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">reviews</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/llm_summarize.html">llm_summarize</a></span><span class="op">(</span><span class="va">review</span>, max_words <span class="op">=</span> <span class="fl">5</span>, pred_name <span class="op">=</span> <span class="st">"review_summary"</span><span class="op">)</span></span>
<span><span class="co">#&gt; # A tibble: 3 × 2</span></span>
<span><span class="co">#&gt;   review                                   review_summary                 </span></span>
<span><span class="co">#&gt;   &lt;chr&gt;                                    &lt;chr&gt;                          </span></span>
<span><span class="co">#&gt; 1 This has been the best TV I've ever use… very good tv experience overall</span></span>
<span><span class="co">#&gt; 2 I regret buying this laptop. It is too … slow and noisy laptop purchase </span></span>
<span><span class="co">#&gt; 3 Not sure how to feel about my new washi… mixed feelings about new washer</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="classify">Classify<a class="anchor" aria-label="anchor" href="#classify"></a>
</h3>
<p>Use the LLM to categorize the text into one of the options you provide:</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">reviews</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/llm_classify.html">llm_classify</a></span><span class="op">(</span><span class="va">review</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"appliance"</span>, <span class="st">"computer"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; # A tibble: 3 × 2</span></span>
<span><span class="co">#&gt;   review                                   .classify</span></span>
<span><span class="co">#&gt;   &lt;chr&gt;                                    &lt;chr&gt;    </span></span>
<span><span class="co">#&gt; 1 This has been the best TV I've ever use… appliance</span></span>
<span><span class="co">#&gt; 2 I regret buying this laptop. It is too … computer </span></span>
<span><span class="co">#&gt; 3 Not sure how to feel about my new washi… appliance</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="extract">Extract<a class="anchor" aria-label="anchor" href="#extract"></a>
</h3>
<p>One of the most interesting operations. Using natural language, we can tell the LLM to return a specific part of the text. In the following example, we request that the LLM return the product being referred to. We do this by simply saying “product”. The LLM understands what we <em>mean</em> by that word, and looks for that in the text.</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">reviews</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/llm_extract.html">llm_extract</a></span><span class="op">(</span><span class="va">review</span>, <span class="st">"product"</span><span class="op">)</span></span>
<span><span class="co">#&gt; # A tibble: 3 × 2</span></span>
<span><span class="co">#&gt;   review                                   .extract       </span></span>
<span><span class="co">#&gt;   &lt;chr&gt;                                    &lt;chr&gt;          </span></span>
<span><span class="co">#&gt; 1 This has been the best TV I've ever use… tv             </span></span>
<span><span class="co">#&gt; 2 I regret buying this laptop. It is too … laptop         </span></span>
<span><span class="co">#&gt; 3 Not sure how to feel about my new washi… washing machine</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="translate">Translate<a class="anchor" aria-label="anchor" href="#translate"></a>
</h3>
<p>As the title implies, this function will translate the text into a specified language. What is really nice, it is that you don’t need to specify the language of the source text. Only the target language needs to be defined. The translation accuracy will depend on the LLM</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">reviews</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/llm_translate.html">llm_translate</a></span><span class="op">(</span><span class="va">review</span>, <span class="st">"spanish"</span><span class="op">)</span></span>
<span><span class="co">#&gt; # A tibble: 3 × 2</span></span>
<span><span class="co">#&gt;   review                                   .translation                         </span></span>
<span><span class="co">#&gt;   &lt;chr&gt;                                    &lt;chr&gt;                                </span></span>
<span><span class="co">#&gt; 1 This has been the best TV I've ever use… Este ha sido el mejor televisor que …</span></span>
<span><span class="co">#&gt; 2 I regret buying this laptop. It is too … Lamento haber comprado esta laptop. …</span></span>
<span><span class="co">#&gt; 3 Not sure how to feel about my new washi… No estoy seguro de cómo sentirme sob…</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="custom-prompt">Custom prompt<a class="anchor" aria-label="anchor" href="#custom-prompt"></a>
</h3>
<p>It is possible to pass your own prompt to the LLM, and have <code>mall</code> run it against each text entry. Use <code><a href="reference/llm_custom.html">llm_custom()</a></code> to access this functionality:</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">my_prompt</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste</a></span><span class="op">(</span></span>
<span>  <span class="st">"Answer a question."</span>,</span>
<span>  <span class="st">"Return only the answer, no explanation"</span>,</span>
<span>  <span class="st">"Acceptable answers are 'yes', 'no'"</span>,</span>
<span>  <span class="st">"Answer this about the following text, is this a happy customer?:"</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">reviews</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/llm_custom.html">llm_custom</a></span><span class="op">(</span><span class="va">review</span>, <span class="va">my_prompt</span><span class="op">)</span></span>
<span><span class="co">#&gt; # A tibble: 3 × 2</span></span>
<span><span class="co">#&gt;   review                                   .pred</span></span>
<span><span class="co">#&gt;   &lt;chr&gt;                                    &lt;chr&gt;</span></span>
<span><span class="co">#&gt; 1 This has been the best TV I've ever use… Yes  </span></span>
<span><span class="co">#&gt; 2 I regret buying this laptop. It is too … No   </span></span>
<span><span class="co">#&gt; 3 Not sure how to feel about my new washi… No</span></span></code></pre></div>
</div>
</div>
<div class="section level2">
<h2 id="initialize-session">Initialize session<a class="anchor" aria-label="anchor" href="#initialize-session"></a>
</h2>
<p>Invoking an <code>llm_</code> function will automatically initialize a model selection if you don’t have one selected yet. If there is only one option, it will pre-select it for you. If there are more than one available models, then <code>mall</code> will present you as menu selection so you can select which model you wish to use.</p>
<p>Calling <code><a href="reference/llm_use.html">llm_use()</a></code> directly will let you specify the model and backend to use. You can also setup additional arguments that will be passed down to the function that actually runs the prediction. In the case of Ollama, that function is <a href="https://hauselin.github.io/ollama-r/reference/generate.html" class="external-link"><code>generate()</code></a>.</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="reference/llm_use.html">llm_use</a></span><span class="op">(</span><span class="st">"ollama"</span>, <span class="st">"llama3.1"</span>, seed <span class="op">=</span> <span class="fl">100</span>, temperature <span class="op">=</span> <span class="fl">0.2</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="key-considerations">Key considerations<a class="anchor" aria-label="anchor" href="#key-considerations"></a>
</h2>
<p>The main consideration is <strong>cost</strong>. Either, time cost, or money cost.</p>
<p>If using this method with an LLM locally available, the cost will be a long running time. Unless using a very specialized LLM, a given LLM is a general model. It was fitted using a vast amount of data. So determining a response for each row, takes longer than if using a manually created NLP model. The default model used in Ollama is Llama 3.1, which was fitted using 8B parameters.</p>
<p>If using an external LLM service, the consideration will need to be for the billing costs of using such service. Keep in mind that you will be sending a lot of data to be evaluated.</p>
<p>Another consideration is the novelty of this approach. Early tests are providing encouraging results. But you, as an user, will still need to keep in mind that the predictions will not be infallible, so always check the output. At this time, I think the best use for this method, is for a quick analysis.</p>
</div>
<div class="section level2">
<h2 id="performance">Performance<a class="anchor" aria-label="anchor" href="#performance"></a>
</h2>
<p>We will briefly cover this methods performance from two perspectives:</p>
<ul>
<li><p>How long the analysis takes to run locally</p></li>
<li><p>How well it predicts</p></li>
</ul>
<p>To do so, we will use the <code>data_bookReviews</code> data set, provided by the <code>classmap</code> package. For this exercise, only the first 100, of the total 1,000, are going to be part of this analysis.</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://doi.org/10.1080/00401706.2021.1927849" class="external-link">classmap</a></span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="va">data_bookReviews</span><span class="op">)</span></span>
<span></span>
<span><span class="va">book_reviews</span> <span class="op">&lt;-</span> <span class="va">data_bookReviews</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="fl">100</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://tibble.tidyverse.org/reference/as_tibble.html" class="external-link">as_tibble</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://pillar.r-lib.org/reference/glimpse.html" class="external-link">glimpse</a></span><span class="op">(</span><span class="va">book_reviews</span><span class="op">)</span></span>
<span><span class="co">#&gt; Rows: 100</span></span>
<span><span class="co">#&gt; Columns: 2</span></span>
<span><span class="co">#&gt; $ review    &lt;chr&gt; "i got this as both a book and an audio file. i had waited t…</span></span>
<span><span class="co">#&gt; $ sentiment &lt;fct&gt; 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 1, …</span></span></code></pre></div>
<p>As per the docs, <code>sentiment</code> is a factor indicating the sentiment of the review: negative (1) or positive (2)</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/strsplit.html" class="external-link">strsplit</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste</a></span><span class="op">(</span><span class="va">book_reviews</span>, collapse <span class="op">=</span> <span class="st">" "</span><span class="op">)</span>, <span class="st">" "</span><span class="op">)</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 20571</span></span></code></pre></div>
<p>Just to get an idea of how much data we’re processing, I’m using a very, very simple word count. So we’re analyzing a bit over 20 thousand words.</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">reviews_llm</span> <span class="op">&lt;-</span> <span class="va">book_reviews</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/llm_sentiment.html">llm_sentiment</a></span><span class="op">(</span></span>
<span>    col <span class="op">=</span> <span class="va">review</span>,</span>
<span>    options <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"positive"</span>, <span class="st">"negative"</span><span class="op">)</span>,</span>
<span>    pred_name <span class="op">=</span> <span class="st">"predicted"</span></span>
<span>  <span class="op">)</span></span>
<span><span class="co">#&gt; ! There were 1 predictions with invalid output, they were coerced to NA</span></span></code></pre></div>
<p>As far as <strong>time</strong>, on my Apple M3 machine, it took about 3 minutes to process, 100 rows, containing 20 thousand words. Setting <code>temp</code> to 0.2 in <code><a href="reference/llm_use.html">llm_use()</a></code>, made the model run a bit faster.</p>
<p>The package uses <code>purrr</code> to send each prompt individually to the LLM. But, I did try a few different ways to speed up the process, unsuccessfully:</p>
<ul>
<li><p>Used <code>furrr</code> to send multiple requests at a time. This did not work because either the LLM or Ollama processed all my requests serially. So there was no improvement.</p></li>
<li><p>I also tried sending more than one row’s text at a time. This cause instability in the number of results. For example sending 5 at a time, sometimes returned 7 or 8. Even sending 2 was not stable.</p></li>
</ul>
<p>This is what the new table looks like:</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">reviews_llm</span></span>
<span><span class="co">#&gt; # A tibble: 100 × 3</span></span>
<span><span class="co">#&gt;    review                                                    sentiment predicted</span></span>
<span><span class="co">#&gt;    &lt;chr&gt;                                                     &lt;fct&gt;     &lt;chr&gt;    </span></span>
<span><span class="co">#&gt;  1 "i got this as both a book and an audio file. i had wait… 1         negative </span></span>
<span><span class="co">#&gt;  2 "this book places too much emphasis on spending money in… 1         negative </span></span>
<span><span class="co">#&gt;  3 "remember the hollywood blacklist? the hollywood ten? i'… 2         negative </span></span>
<span><span class="co">#&gt;  4 "while i appreciate what tipler was attempting to accomp… 1         negative </span></span>
<span><span class="co">#&gt;  5 "the others in the series were great, and i really looke… 1         negative </span></span>
<span><span class="co">#&gt;  6 "a few good things, but she's lost her edge and i find i… 1         negative </span></span>
<span><span class="co">#&gt;  7 "words cannot describe how ripped off and disappointed i… 1         negative </span></span>
<span><span class="co">#&gt;  8 "1. the persective of most writers is shaped by their ow… 1         negative </span></span>
<span><span class="co">#&gt;  9 "i have been a huge fan of michael crichton for about 25… 1         negative </span></span>
<span><span class="co">#&gt; 10 "i saw dr. polk on c-span a month or two ago. he was add… 2         positive </span></span>
<span><span class="co">#&gt; # ℹ 90 more rows</span></span></code></pre></div>
<p>I used <code>yardstick</code> to see how well the model performed. Of course, the accuracy will not be of the “truth”, but rather the package’s results recorded in <code>sentiment</code>.</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://forcats.tidyverse.org/" class="external-link">forcats</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">reviews_llm</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html" class="external-link">mutate</a></span><span class="op">(</span>fct_pred <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html" class="external-link">as.factor</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/ifelse.html" class="external-link">ifelse</a></span><span class="op">(</span><span class="va">predicted</span> <span class="op">==</span> <span class="st">"positive"</span>, <span class="fl">2</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">yardstick</span><span class="fu">::</span><span class="fu">accuracy</span><span class="op">(</span><span class="va">sentiment</span>, <span class="va">fct_pred</span><span class="op">)</span></span>
<span><span class="co">#&gt; # A tibble: 1 × 3</span></span>
<span><span class="co">#&gt;   .metric  .estimator .estimate</span></span>
<span><span class="co">#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;</span></span>
<span><span class="co">#&gt; 1 accuracy binary         0.939</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="vector-functions">Vector functions<a class="anchor" aria-label="anchor" href="#vector-functions"></a>
</h2>
<p><code>mall</code> includes functions that expect a vector, instead of a table, to run the predictions. This should make it easier to test things, such as custom prompts or results of specific text. Each <code>llm_</code> function has a corresponding <code>llm_vec_</code> function:</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="reference/llm_sentiment.html">llm_vec_sentiment</a></span><span class="op">(</span><span class="st">"I am happy"</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "positive"</span></span></code></pre></div>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="reference/llm_translate.html">llm_vec_translate</a></span><span class="op">(</span><span class="st">"Este es el mejor dia!"</span>, <span class="st">"english"</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "This is the best day!"</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="databricks">Databricks<a class="anchor" aria-label="anchor" href="#databricks"></a>
</h2>
<p>This brief example shows how seamless it is to use the same <code>llm_</code> functions, but against a remote connection:</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://dbi.r-dbi.org" class="external-link">DBI</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">con</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://dbi.r-dbi.org/reference/dbConnect.html" class="external-link">dbConnect</a></span><span class="op">(</span></span>
<span>  <span class="fu">odbc</span><span class="fu">::</span><span class="fu">databricks</span><span class="op">(</span><span class="op">)</span>,</span>
<span>  HTTPPath <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Sys.getenv.html" class="external-link">Sys.getenv</a></span><span class="op">(</span><span class="st">"DATABRICKS_PATH"</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">tbl_reviews</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/copy_to.html" class="external-link">copy_to</a></span><span class="op">(</span><span class="va">con</span>, <span class="va">reviews</span><span class="op">)</span></span></code></pre></div>
<p>As mentioned above, using <code><a href="reference/llm_sentiment.html">llm_sentiment()</a></code> in Databricks will call that vendor’s SQL AI function directly:</p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">tbl_reviews</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/llm_sentiment.html">llm_sentiment</a></span><span class="op">(</span><span class="va">review</span><span class="op">)</span></span>
<span><span class="co">#&gt; # Source:   SQL [3 x 2]</span></span>
<span><span class="co">#&gt; # Database: Spark SQL 3.1.1[token@Spark SQL/hive_metastore]</span></span>
<span><span class="co">#&gt;   review                                                              .sentiment</span></span>
<span><span class="co">#&gt;   &lt;chr&gt;                                                               &lt;chr&gt;     </span></span>
<span><span class="co">#&gt; 1 This has been the best TV Ive ever used. Great screen, and sound.   positive  </span></span>
<span><span class="co">#&gt; 2 I regret buying this laptop. It is too slow and the keyboard is to… negative  </span></span>
<span><span class="co">#&gt; 3 Not sure how to feel about my new washing machine. Great color, bu… mixed</span></span></code></pre></div>
<p>There are some differences in the arguments, and output of the LLM’s. Notice that instead of “neutral”, the prediction is “mixed”. The AI Sentiment function does not allow to change the possible options.</p>
<p>Next, we will try <code><a href="reference/llm_summarize.html">llm_summarize()</a></code>. The <code>max_words</code> argument maps to the same argument in the AI Summarize function:</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">tbl_reviews</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="reference/llm_summarize.html">llm_summarize</a></span><span class="op">(</span><span class="va">review</span>, max_words <span class="op">=</span> <span class="fl">5</span><span class="op">)</span></span>
<span><span class="co">#&gt; # Source:   SQL [3 x 2]</span></span>
<span><span class="co">#&gt; # Database: Spark SQL 3.1.1[token@Spark SQL/hive_metastore]</span></span>
<span><span class="co">#&gt;   review                                                                .summary</span></span>
<span><span class="co">#&gt;   &lt;chr&gt;                                                                 &lt;chr&gt;   </span></span>
<span><span class="co">#&gt; 1 This has been the best TV Ive ever used. Great screen, and sound.     Superio…</span></span>
<span><span class="co">#&gt; 2 I regret buying this laptop. It is too slow and the keyboard is too … Slow, n…</span></span>
<span><span class="co">#&gt; 3 Not sure how to feel about my new washing machine. Great color, but … Initial…</span></span></code></pre></div>
</div>
</div>
  </main><aside class="col-md-3"><div class="links">
<h2 data-toc-skip>Links</h2>
<ul class="list-unstyled">
<li><a href="https://github.com/edgararuiz/mall" class="external-link">Browse source code</a></li>
</ul>
</div>

<div class="license">
<h2 data-toc-skip>License</h2>
<ul class="list-unstyled">
<li><a href="LICENSE.html">Full license</a></li>
<li><small><a href="https://opensource.org/licenses/mit-license.php" class="external-link">MIT</a> + file <a href="LICENSE-text.html">LICENSE</a></small></li>
</ul>
</div>


<div class="citation">
<h2 data-toc-skip>Citation</h2>
<ul class="list-unstyled">
<li><a href="authors.html#citation">Citing mall</a></li>
</ul>
</div>

<div class="developers">
<h2 data-toc-skip>Developers</h2>
<ul class="list-unstyled">
<li>Edgar Ruiz <br><small class="roles"> Author, maintainer </small>  </li>
</ul>
</div>

<div class="dev-status">
<h2 data-toc-skip>Dev status</h2>
<ul class="list-unstyled">
<li><a href="https://github.com/edgararuiz/mall/actions/workflows/R-CMD-check.yaml" class="external-link"><img src="https://github.com/edgararuiz/mall/actions/workflows/R-CMD-check.yaml/badge.svg" alt="R-CMD-check"></a></li>
<li><a href="https://lifecycle.r-lib.org/articles/stages.html#experimental" class="external-link"><img src="https://img.shields.io/badge/lifecycle-experimental-orange.svg" alt="Lifecycle: experimental"></a></li>
<li><a href="https://app.codecov.io/gh/edgararuiz/mall?branch=main" class="external-link"><img src="https://codecov.io/gh/edgararuiz/mall/branch/main/graph/badge.svg" alt="Codecov test coverage"></a></li>
</ul>
</div>

  </aside>
</div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Edgar Ruiz.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.0.</p>
</div>

    </footer>
</div>





  </body>
</html>
